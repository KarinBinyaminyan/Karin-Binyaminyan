{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmu6QdWzbi-H",
        "outputId": "af1f6647-ea5e-472d-fd7f-8dc030851ded"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Imports:**"
      ],
      "metadata": {
        "id": "G2p6wEr8b-lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, roc_auc_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "u2ek_mt3bi4f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Prepare data preprocessing transformations:**"
      ],
      "metadata": {
        "id": "WfY6Y2i1cHlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_train_data = \"/content/gdrive/MyDrive/karin/Car_Brand_Logos/Train\"\n",
        "path_test_data = \"/content/gdrive/MyDrive/karin/Car_Brand_Logos/Test\"\n",
        "\n",
        "\n",
        "transform_for_std_mean = transforms.Compose([transforms.Resize((64, 64)),transforms.ToTensor()])\n",
        "\n",
        "# Calculate mean and std of the training dataset\n",
        "train_dataset = ImageFolder(root=path_train_data, transform=transform_for_std_mean)\n",
        "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "mean = torch.zeros(3)\n",
        "std = torch.zeros(3)\n",
        "\n",
        "for inputs, _ in train_loader:\n",
        "    mean += inputs.mean(dim=(0, 2, 3))\n",
        "    std += inputs.std(dim=(0, 2, 3))\n",
        "\n",
        "mean /= len(train_loader)\n",
        "std /= len(train_loader)\n",
        "print(\"mean of the dataset is: \", mean)\n",
        "print(\"std of the dataset is: \", std)\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), shear=15, scale=(0.9, 1.1)),  # Shear and zoom\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean.tolist(), std.tolist())\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean.tolist(), std.tolist())\n",
        "])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_kWq9Zzbi0x",
        "outputId": "9c953336-16ef-470d-e779-882c17918034"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean of the dataset is:  tensor([0.5400, 0.5392, 0.5472])\n",
            "std of the dataset is:  tensor([0.3702, 0.3649, 0.3666])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the dataloaders and set the device (cuda/cpu)"
      ],
      "metadata": {
        "id": "BZ9tbzpScmvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# train_dataset = ImageFolder(root='/content/gdrive/MyDrive/karin/Car_Brand_Logos/Train', transform=transform)\n",
        "# test_dataset = ImageFolder(root='/content/gdrive/MyDrive/karin/Car_Brand_Logos/Test', transform=transform)\n",
        "train_dataset = ImageFolder(root=path_train_data, transform=transform)\n",
        "test_dataset = ImageFolder(root=path_test_data,  transform=transform_test)\n",
        "# Split train dataset into train and validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=1)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256,num_workers=1)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "nXBS0wI_bisN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot some examples of the augmented data"
      ],
      "metadata": {
        "id": "xmxmfGWHcs-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a few images from the dataset\n",
        "num_images_to_display = 5\n",
        "data_loader = DataLoader(train_dataset, batch_size=num_images_to_display, shuffle=True)\n",
        "images, _ = next(iter(data_loader))\n",
        "\n",
        "# Denormalize the images\n",
        "denormalize = transforms.Compose([\n",
        "    transforms.Normalize((-1, -1, -1), (2, 2, 2))\n",
        "])\n",
        "\n",
        "denorm_images = []\n",
        "for i in range(num_images_to_display):\n",
        "    denorm_image = denormalize(images[i])\n",
        "    denorm_images.append(denorm_image)\n",
        "\n",
        "# Display the images\n",
        "fig, axes = plt.subplots(1, num_images_to_display, figsize=(15, 3))\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(np.transpose(denorm_images[i], (1, 2, 0)))\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Phnlrklwbify"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "nrxJqoy1bh2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 16 * 16)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleCNNModelWithDropout(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.25):\n",
        "        super(SimpleCNNModelWithDropout, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 8)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 32 * 16 * 16)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleCNNModelWithBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNNModelWithBN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = x.view(-1, 32 * 16 * 16)\n",
        "        x = F.relu(self.bn3(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleCNNModelWithAll(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.25):\n",
        "        super(SimpleCNNModelWithAll, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 8)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # self.l2_strength = l2_strength\n",
        "        # self.register_buffer('l1_reg', torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 32 * 16 * 16)\n",
        "        x = F.relu(self.bn3(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5mn6uUUGdxz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train All Models"
      ],
      "metadata": {
        "id": "qo2dDyDfefQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, model_name, learning_rate=0.001, epochs=100, regularization=None,\n",
        "                    regularization_strength=None):\n",
        "      # Define loss and optimizer\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      if regularization == None or regularization == \"l1\":\n",
        "          optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "      elif regularization == \"l2\":\n",
        "          optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization_strength)\n",
        "\n",
        "      if regularization != None:\n",
        "          print(\"Training the model: \", model_name, \" with \", regularization, \" regularization\")\n",
        "      else:\n",
        "          print(\"Training the model: \", model_name)\n",
        "\n",
        "      train_losses = []  # To store training losses for each epoch\n",
        "      val_accuracies = []  # To store validation accuracies for each epoch\n",
        "      val_losses = []\n",
        "      best_val_acc = 0\n",
        "      best_val_loss = 100\n",
        "      best_acc_epoch = 0\n",
        "      best_loss_epoch = 0\n",
        "      model.to(device)\n",
        "      # Training loop\n",
        "      for epoch in range(epochs):\n",
        "          model.train()\n",
        "          epoch_train_loss = 0.0  # To accumulate training loss\n",
        "\n",
        "          for batch_index, (inputs, labels) in enumerate(train_loader):\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              optimizer.zero_grad()\n",
        "              outputs = model(inputs)\n",
        "\n",
        "              loss = criterion(outputs, labels)\n",
        "\n",
        "              if regularization == \"l1\":\n",
        "                  l1_lambda = regularization_strength  # Adjust this value for the strength of L1 regularization\n",
        "                  l1_regularization = torch.tensor(0.).to(device)\n",
        "                  for param in model.parameters():\n",
        "                      l1_regularization += torch.norm(param, p=1)\n",
        "                  loss += l1_lambda * l1_regularization\n",
        "\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              # Accumulate training loss for the epoch\n",
        "              epoch_train_loss += loss.item()\n",
        "\n",
        "          # Validation\n",
        "          model.eval()\n",
        "          val_loss = 0.0\n",
        "          correct = 0\n",
        "          total = 0\n",
        "          with torch.no_grad():\n",
        "              for inputs, labels in val_loader:\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "                  outputs = model(inputs)\n",
        "                  val_loss += criterion(outputs, labels).item()\n",
        "                  _, predicted = outputs.max(1)\n",
        "                  total += labels.size(0)\n",
        "                  correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "          val_accuracy = 100.0 * correct / total\n",
        "          average_train_loss = epoch_train_loss / len(train_loader)  # Calculate average training loss\n",
        "\n",
        "          print(\n",
        "              f'Epoch [{epoch + 1}/{epochs}], Train Loss: {average_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "          if val_accuracy > best_val_acc:\n",
        "              best_val_acc = val_accuracy\n",
        "              # Save model weights\n",
        "              torch.save(model.state_dict(), \"./\" + model_name + \"_weights.pth\")\n",
        "              best_acc_epoch = epoch + 1\n",
        "\n",
        "          # if val_loss < best_val_loss:\n",
        "          #     best_val_loss = val_loss\n",
        "          #     # Save model weights\n",
        "          #     torch.save(model.state_dict(), \"./\" + model_name + \"_weights.pth\")\n",
        "          #     best_loss_epoch = epoch + 1\n",
        "\n",
        "          train_losses.append(average_train_loss)\n",
        "          val_accuracies.append(val_accuracy)\n",
        "          val_losses.append(val_loss)\n",
        "      print(\"best weights model saved on: epoch \", best_acc_epoch)\n",
        "      print(\"From the next epoch, overfit start to occur: epoch \", best_loss_epoch)\n",
        "\n",
        "      return train_losses, val_accuracies, val_losses\n",
        "\n",
        "print(\"device: \", device)\n",
        "print(len(train_dataset))\n",
        "# Train each model\n",
        "model_names = ['cnn_baseline', 'cnn_bn', 'cnn_dropout', 'cnn_all']\n",
        "models = [SimpleCNNModel(), SimpleCNNModelWithBN(), SimpleCNNModelWithDropout(), SimpleCNNModelWithAll()]\n",
        "epochs = 200\n",
        "\n",
        "all_train_losses = []\n",
        "all_val_accuracies = []\n",
        "all_val_losses = []\n",
        "\n",
        "for model, model_name in zip(models, model_names):\n",
        "    train_losses, val_accuracies, val_losses = train_model(model, train_loader, val_loader, model_name=model_name,\n",
        "                                                            epochs=epochs)\n",
        "    all_train_losses.append(train_losses)\n",
        "    all_val_accuracies.append(val_accuracies)\n",
        "    all_val_losses.append(val_losses)\n",
        "\n",
        "# L2 regularization\n",
        "model_name = \"cnn_all_l2_regularization\"\n",
        "model = SimpleCNNModelWithAll()\n",
        "model_names.append(model_name)\n",
        "\n",
        "train_losses_l2, val_accuracies_l2, val_losses = train_model(model, train_loader, val_loader, model_name=model_name,\n",
        "                                                              epochs=epochs, regularization=\"l2\",\n",
        "                                                              regularization_strength=0.001)\n",
        "all_train_losses.append(train_losses_l2)\n",
        "all_val_accuracies.append(val_accuracies_l2)\n",
        "all_val_losses.append(val_losses)\n",
        "\n",
        "# L1 regularization\n",
        "model_name = \"cnn_all_l1_regularization\"\n",
        "model = SimpleCNNModelWithAll()\n",
        "model_names.append(model_name)\n",
        "\n",
        "train_losses_l1, val_accuracies_l1, val_losses = train_model(model, train_loader, val_loader, model_name=model_name,\n",
        "                                                              epochs=epochs, regularization=\"l1\",\n",
        "                                                              regularization_strength=0.001)\n",
        "all_train_losses.append(train_losses_l1)\n",
        "all_val_accuracies.append(val_accuracies_l1)\n",
        "all_val_losses.append(val_losses)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "1otVv6oSeIL4",
        "outputId": "873c50d1-58e1-412c-ee07-e75f83fc4c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cuda\n",
            "2010\n",
            "Training the model:  cnn_baseline\n",
            "Epoch [1/200], Train Loss: 2.1031, Val Loss: 4.1263, Val Acc: 15.90%\n",
            "Epoch [2/200], Train Loss: 2.0508, Val Loss: 4.0615, Val Acc: 21.87%\n",
            "Epoch [3/200], Train Loss: 2.0141, Val Loss: 3.9839, Val Acc: 28.23%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-61dcb68b1f47>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     train_losses, val_accuracies, val_losses = train_model(model, train_loader, val_loader, model_name=model_name,\n\u001b[0m\u001b[1;32m    102\u001b[0m                                                             epochs=epochs)\n\u001b[1;32m    103\u001b[0m     \u001b[0mall_train_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-61dcb68b1f47>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, model_name, learning_rate, epochs, regularization, regularization_strength)\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m  \u001b[0;31m# To accumulate training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Validation loss and accuracy graphs"
      ],
      "metadata": {
        "id": "zKg19CZufk31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Training and Validations Graphs for the different models\"\"\"\n",
        "\n",
        "# Plot validation graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, model_name in enumerate(model_names):\n",
        "    if len(all_val_accuracies) > i:\n",
        "        plt.plot(range(1, epochs + 1), all_val_accuracies[i], label=model_name + \"_val\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.savefig(\"validation_accuracy.png\")\n",
        "\n",
        "# Plot validation loss graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, model_name in enumerate(model_names):\n",
        "    if len(all_val_accuracies) > i:\n",
        "        plt.plot(range(1, epochs + 1), all_val_losses[i], label=model_name + \"_val_loss\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.savefig(\"validation_loss.png\")\n",
        "\n",
        "\n",
        "# Plot training graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, model_name in enumerate(model_names):\n",
        "    if len(all_val_accuracies) > i:\n",
        "        plt.plot(range(1, epochs + 1), all_train_losses[i], label=model_name + \"_train\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.savefig(\"training_loss.png\")\n",
        "\n",
        "\n",
        "# Plot training graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, model_name in enumerate(model_names):\n",
        "    if len(all_val_accuracies) > i and \"l1\" not in model_name:\n",
        "        plt.plot(range(1, epochs + 1), all_train_losses[i], label=model_name + \"_train\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.savefig(\"training_loss_without_l1.png\")\n"
      ],
      "metadata": {
        "id": "D6H1aMsZfkLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the final models"
      ],
      "metadata": {
        "id": "sI7tdScdhVQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Models Evaluation on the test data\"\"\"\n",
        "\n",
        "def load_model(model, model_name):\n",
        "    print(model_name)\n",
        "    model.load_state_dict(torch.load(\"./\" + model_name + \"_weights.pth\"))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model_names = ['cnn_baseline', 'cnn_bn', 'cnn_dropout', 'cnn_all']\n",
        "models = [SimpleCNNModel(), SimpleCNNModelWithBN(), SimpleCNNModelWithDropout(), SimpleCNNModelWithAll()]\n",
        "# Load the trained models\n",
        "loaded_models = []\n",
        "\n",
        "model_name = \"cnn_all_l1_regularization\"\n",
        "model = SimpleCNNModelWithAll()\n",
        "model_names.append(model_name)\n",
        "models.append(model)\n",
        "model_name = \"cnn_all_l2_regularization\"\n",
        "model = SimpleCNNModelWithAll()\n",
        "model_names.append(model_name)\n",
        "models.append(model)\n",
        "\n",
        "for model, model_name in zip(models, model_names):\n",
        "    model = load_model(model, model_name)\n",
        "    loaded_models.append(model)"
      ],
      "metadata": {
        "id": "tdGkHnCAhVh-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "2707070d-aff7-4aa3-dc00-fdd3869602ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn_baseline\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7c9ab4e87715>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mloaded_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7c9ab4e87715>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model, model_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_weights.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cnn_baseline_weights.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models evaluation:"
      ],
      "metadata": {
        "id": "7C2Gr4r9fu9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy = 0\n",
        "best_model = \"\"\n",
        "for model, model_name in zip(loaded_models, model_names):\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100.0 * correct / total\n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "        best_model = model_name\n",
        "    print(f'Model {model_name} - Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "print(f'Best Model on test dataset {best_model} - with Test Accuracy: {best_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "huIL1dkCfuaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37cb6dd0-dc7c-45cb-997d-70ca3e2f445d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model on test dataset  - with Test Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Graph for the best model (Confusion Matrix)"
      ],
      "metadata": {
        "id": "cR78GScsf9UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy, all_labels, all_predictions\n",
        "\n",
        "model_name = \"cnn_all_l2_regularization\"\n",
        "model = SimpleCNNModelWithAll()\n",
        "model = load_model(model, model_name)\n",
        "# After loading the trained models\n",
        "model.to(device)\n",
        "# After loading the trained models\n",
        "# After loading the trained models\n",
        "test_accuracy, true_labels, predicted_labels = evaluate_model(model, test_loader)\n",
        "\n",
        "print(f'Model {model_name} - Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(f'Confusion Matrix for {model_name}:\\n', cm)\n",
        "\n",
        "\n",
        "\n",
        "class_names = test_dataset.classes\n",
        "\n",
        "TP = cm.diagonal()\n",
        "FP = cm.sum(axis=0) - TP\n",
        "FN = cm.sum(axis=1) - TP\n",
        "TN = cm.sum() - (TP + FP + FN)\n",
        "\n",
        "# Calculate recall (Sensitivity) for each class\n",
        "recall = TP / (TP + FN)\n",
        "\n",
        "# Calculate precision for each class\n",
        "precision = TP / (TP + FP)\n",
        "\n",
        "# Calculate F1-score for each class\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "# Create a DataFrame to organize the metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Class': class_names,\n",
        "    'Recall (Sensitivity)': recall,\n",
        "    'Precision': precision,\n",
        "    'F1-Score': f1\n",
        "})\n",
        "\n",
        "# Save the metrics table as an image (e.g., PNG)\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "plt.axis('off')  # Hide axes\n",
        "table = plt.table(cellText=metrics_df.values,\n",
        "                  colLabels=metrics_df.columns,\n",
        "                  cellLoc='center',\n",
        "                  loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)  # Adjust font size as needed\n",
        "plt.title('Metrics Table')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the table as an image (e.g., PNG)\n",
        "plt.savefig('metrics_table.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot the confusion matrix using ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(cmap=plt.cm.Blues,  values_format='d')\n",
        "plt.xticks(fontsize=7)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.title(f'Confusion Matrix for {model_name}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KlVgoC4Nf9EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOT PRACTICAL JUST FOR DEMONSTRATION\n",
        "Grid Search for all models, but due computional limitations it's not practical, so **the code in a comment.**"
      ],
      "metadata": {
        "id": "qPUPJEqFgMTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"# Grid Search Over Multiple models\n",
        "\n",
        "The code is Proof of Concept that I could do Grid search over multiple models.\n",
        "Due computational limits, the code is in comment.\n",
        "In practice I chose SimpleCNNModelWithAll with L2 regularization because is it combinning multiple strategies for that we learned in the course, dealing with overfit.\n",
        "\"\"\"\n",
        "\n",
        "# # Define hyperparameter values for grid search\n",
        "# dropout_rates = [0.25, 0.5]\n",
        "# regularization_strengths = [0.001, 0.01]\n",
        "# learning_rates = [0.001, 0.01]\n",
        "# batch_sizes = [64, 128]\n",
        "# # train_sizes = [0.6, 0.7, 0.8]  # Proportion of the training dataset to use\n",
        "\n",
        "# model_names = ['cnn_baseline','cnn_bn','cnn_dropout',  'cnn_all','cnn_all_l2_regularization','cnn_all_l1_regularization']\n",
        "# models = [SimpleCNNModel,SimpleCNNModelWithBN, SimpleCNNModelWithDropout, SimpleCNNModelWithAll,SimpleCNNModelWithAll,SimpleCNNModelWithAll]\n",
        "\n",
        "# model_names = ['cnn_all_l1_regularization',  'cnn_all','cnn_all_l2_regularization','cnn_all_l1_regularization']\n",
        "# models = [SimpleCNNModelWithAll, SimpleCNNModelWithAll]\n",
        "\n",
        "# # Dictionary to map model names to their corresponding hyperparameters\n",
        "# model_hyperparams = {\n",
        "#     \"cnn_baseline\": {\"dropout_rate\": None, \"regularization_strength\": None, \"learning_rate\": learning_rates, \"batch_size\": batch_sizes},\n",
        "#     \"cnn_bn\": {\"dropout_rate\": None, \"regularization_strength\": None, \"learning_rate\": learning_rates, \"batch_size\": batch_sizes},\n",
        "#     \"cnn_dropout\": {\"dropout_rate\": dropout_rates, \"regularization_strength\": None, \"learning_rate\": learning_rates, \"batch_size\": batch_sizes},\n",
        "#     \"cnn_all\": {\"dropout_rate\": dropout_rates, \"regularization_strength\": None, \"learning_rate\": learning_rates, \"batch_size\": batch_sizes},\n",
        "#     \"cnn_all_l2_regularization\":{\"dropout_rate\": dropout_rates, \"regularization_strength\": regularization_strengths, \"learning_rate\": learning_rates, \"batch_size\": batch_sizes},\n",
        "#     \"cnn_all_l1_regularization\":{\"dropout_rate\": dropout_rates, \"regularization_strength\": regularization_strengths, \"learning_rate\": learning_rates, \"batch_size\": batch_sizes},\n",
        "# }\n",
        "\n",
        "# # Lists to store best hyperparameters and their corresponding performance metrics\n",
        "# best_hyperparams = []\n",
        "# best_val_accuracies = []\n",
        "\n",
        "# # Nested loop for grid search\n",
        "# for model, model_name in zip(models, model_names):\n",
        "#     hyperparams = model_hyperparams[model_name]\n",
        "#     best_accuracy = 0.0\n",
        "#     best_params = {}\n",
        "\n",
        "#     for dropout_rate in hyperparams[\"dropout_rate\"] if hyperparams[\"dropout_rate\"] else [None]:\n",
        "#         for regularization_strength in hyperparams[\"regularization_strength\"] if hyperparams[\"regularization_strength\"] else [None]:\n",
        "#             for learning_rate in hyperparams[\"learning_rate\"]:\n",
        "#                 for batch_size in hyperparams[\"batch_size\"]:\n",
        "#                           # Create train and validation loaders with the current dataset size\n",
        "#                           train_size_idx = int(train_size * len(train_dataset))\n",
        "#                           train_loader_subset = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#                           val_loader_subset = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "#                           # Create a new instance of the model with the current hyperparameters\n",
        "#                           if dropout_rate:\n",
        "#                               model_instance = model(dropout_rate=dropout_rate)\n",
        "#                               print(model_instance.dropout)\n",
        "#                           else:\n",
        "#                               model_instance = model()\n",
        "\n",
        "#                           if \"l1\" in model_name:\n",
        "#                               train_losses, val_accuracies, _ = train_model(model_instance, train_loader_subset, val_loader_subset, model_name=model_name, epochs=epochs, learning_rate=learning_rate,regularization=\"l1\", regularization_strength=regularization_strength)\n",
        "#                           elif \"l2\" in model_name: # \"l2\"\n",
        "#                               train_losses, val_accuracies, _ = train_model(model_instance, train_loader_subset, val_loader_subset, model_name=model_name, epochs=epochs, learning_rate=learning_rate,regularization=\"l2\", regularization_strength=regularization_strength)\n",
        "#                           else:\n",
        "#                               train_losses, val_accuracies, _ = train_model(model_instance, train_loader_subset, val_loader_subset, model_name=model_name, epochs=epochs, learning_rate=learning_rate)\n",
        "\n",
        "#                           final_val_accuracy = val_accuracies[-1]\n",
        "\n",
        "#                           if final_val_accuracy > best_accuracy:\n",
        "#                               best_accuracy = final_val_accuracy\n",
        "#                               best_params = {\"dropout_rate\": dropout_rate, \"regularization_strength\": regularization_strength, \"learning_rate\": learning_rate, \"batch_size\": batch_size, \"train_size\": train_size, \"epochs\": epochs}\n",
        "\n",
        "#     best_hyperparams.append(best_params)\n",
        "#     best_val_accuracies.append(best_accuracy)\n",
        "\n",
        "# # Print best hyperparameters and corresponding val accuracies\n",
        "# for model_name, best_params, best_accuracy in zip(model_names, best_hyperparams, best_val_accuracies):\n",
        "#     print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
        "#     print(f\"Best validation accuracy: {best_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "DAC2hiEkgK8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Grid Search for the best model.\n",
        "It's practical to run it but gonna take sometime, so it's unnecesarry, I saved the best parameters from my previous running. read it on my report.\n"
      ],
      "metadata": {
        "id": "pxoZYafMgY7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"# Grid Search for the best model\n",
        "\n",
        "# \"\"\"\n",
        "# dropout_rates = [0.25, 0.5]\n",
        "# regularization_strengths = [0.0001, 0.001]\n",
        "# learning_rates = [0.001, 0.01]\n",
        "# batch_sizes = [128, 256]\n",
        "# epochs = 30\n",
        "\n",
        "# # Model and model name for grid search\n",
        "# model = SimpleCNNModelWithAll\n",
        "# model_name = 'cnn_all_l2_regularization_grid_search'\n",
        "# # Lists to store best hyperparameters and their corresponding performance metrics\n",
        "# best_hyperparams = []\n",
        "# best_val_accuracy = 0.0\n",
        "\n",
        "# # Lists to store validation accuracies, validation losses, and training losses for each hyperparameter combination\n",
        "# all_val_accuracies = []\n",
        "# all_val_losses = []\n",
        "# all_train_losses = []\n",
        "# all_hyperparams = []\n",
        "# # Nested loop for grid search\n",
        "# for dropout_rate in dropout_rates:\n",
        "#     for regularization_strength in regularization_strengths:\n",
        "#         for learning_rate in learning_rates:\n",
        "#             for batch_size in batch_sizes:\n",
        "#                 all_hyperparams.append({\"dropout_rate\": dropout_rate, \"regularization_strength\": regularization_strength, \"learning_rate\": learning_rate, \"batch_size\": batch_size})\n",
        "#                 # Create train and validation loaders\n",
        "#                 train_loader_subset = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#                 val_loader_subset = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#                 # Create a new instance of the model with the current hyperparameters\n",
        "#                 model_instance = model(dropout_rate=dropout_rate, l2_strength=regularization_strength)\n",
        "\n",
        "#                 # Train the model\n",
        "#                 train_losses, val_accuracies, val_losses = train_model(model_instance, train_loader_subset,\n",
        "#                                                                         val_loader_subset, model_name=model_name,\n",
        "#                                                                         epochs=epochs, learning_rate=learning_rate)\n",
        "#                 highest_val_accuracy = max(val_accuracies)\n",
        "\n",
        "#                 # Append metrics to lists\n",
        "#                 all_val_accuracies.append(val_accuracies)\n",
        "#                 all_val_losses.append(val_losses)\n",
        "#                 all_train_losses.append(train_losses)\n",
        "\n",
        "#                 if highest_val_accuracy > best_val_accuracy:\n",
        "#                     best_val_accuracy = highest_val_accuracy\n",
        "#                     best_hyperparams = {\"dropout_rate\": dropout_rate,\n",
        "#                                         \"regularization_strength\": regularization_strength,\n",
        "#                                         \"learning_rate\": learning_rate, \"batch_size\": batch_size}\n",
        "\n",
        "\n",
        "# # Print best hyperparameters and corresponding val accuracy\n",
        "# print(f\"Best hyperparameters for {model_name}: {best_hyperparams}\")\n",
        "# print(f\"Highest validation accuracy: {best_val_accuracy:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "woKXr1VegZZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping for best model"
      ],
      "metadata": {
        "id": "kIgRfXOGiSgU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPYDoaAUbaT2",
        "outputId": "0bfb2b3d-d955-4f35-abd2-16437d72cca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model: cnn_early_stopping\n",
            "Epoch [1/200], Train Loss: 2.0867, Val Acc: 15.71%\n",
            "Epoch [2/200], Train Loss: 1.9167, Val Acc: 17.10%\n",
            "Epoch [3/200], Train Loss: 1.8331, Val Acc: 23.66%\n",
            "Epoch [4/200], Train Loss: 1.7681, Val Acc: 29.22%\n",
            "Epoch [5/200], Train Loss: 1.7078, Val Acc: 32.60%\n",
            "Epoch [6/200], Train Loss: 1.6966, Val Acc: 33.80%\n",
            "Epoch [7/200], Train Loss: 1.6410, Val Acc: 38.37%\n",
            "Epoch [8/200], Train Loss: 1.6229, Val Acc: 38.97%\n",
            "Epoch [9/200], Train Loss: 1.5947, Val Acc: 37.57%\n",
            "Epoch [10/200], Train Loss: 1.5564, Val Acc: 39.96%\n",
            "Epoch [11/200], Train Loss: 1.5249, Val Acc: 42.54%\n",
            "Epoch [12/200], Train Loss: 1.5068, Val Acc: 40.36%\n",
            "Epoch [13/200], Train Loss: 1.5229, Val Acc: 42.35%\n",
            "Epoch [14/200], Train Loss: 1.4523, Val Acc: 44.73%\n",
            "Epoch [15/200], Train Loss: 1.4563, Val Acc: 45.73%\n",
            "Epoch [16/200], Train Loss: 1.4231, Val Acc: 44.93%\n",
            "Epoch [17/200], Train Loss: 1.3995, Val Acc: 45.53%\n",
            "Epoch [18/200], Train Loss: 1.3999, Val Acc: 45.13%\n",
            "Epoch [19/200], Train Loss: 1.3734, Val Acc: 47.91%\n",
            "Epoch [20/200], Train Loss: 1.3998, Val Acc: 49.50%\n",
            "Epoch [21/200], Train Loss: 1.3566, Val Acc: 45.73%\n",
            "Epoch [22/200], Train Loss: 1.3364, Val Acc: 45.33%\n",
            "Epoch [23/200], Train Loss: 1.3536, Val Acc: 50.30%\n",
            "Epoch [24/200], Train Loss: 1.3107, Val Acc: 48.31%\n",
            "Epoch [25/200], Train Loss: 1.2884, Val Acc: 49.50%\n",
            "Epoch [26/200], Train Loss: 1.2999, Val Acc: 50.70%\n",
            "Epoch [27/200], Train Loss: 1.2955, Val Acc: 54.08%\n",
            "Epoch [28/200], Train Loss: 1.2701, Val Acc: 49.90%\n",
            "Epoch [29/200], Train Loss: 1.2240, Val Acc: 55.07%\n",
            "Epoch [30/200], Train Loss: 1.2525, Val Acc: 51.89%\n",
            "Epoch [31/200], Train Loss: 1.2455, Val Acc: 49.30%\n",
            "Epoch [32/200], Train Loss: 1.2347, Val Acc: 54.08%\n",
            "Epoch [33/200], Train Loss: 1.1975, Val Acc: 51.69%\n",
            "Epoch [34/200], Train Loss: 1.2344, Val Acc: 52.68%\n",
            "Epoch [35/200], Train Loss: 1.2045, Val Acc: 50.50%\n",
            "Epoch [36/200], Train Loss: 1.2117, Val Acc: 54.87%\n",
            "Epoch [37/200], Train Loss: 1.1920, Val Acc: 53.68%\n",
            "Epoch [38/200], Train Loss: 1.2040, Val Acc: 53.28%\n",
            "Epoch [39/200], Train Loss: 1.1548, Val Acc: 50.50%\n",
            "Epoch [40/200], Train Loss: 1.1632, Val Acc: 53.68%\n",
            "Epoch [41/200], Train Loss: 1.1540, Val Acc: 55.27%\n",
            "Epoch [42/200], Train Loss: 1.1672, Val Acc: 55.86%\n",
            "Epoch [43/200], Train Loss: 1.1755, Val Acc: 55.86%\n",
            "Epoch [44/200], Train Loss: 1.1627, Val Acc: 54.67%\n",
            "Epoch [45/200], Train Loss: 1.1206, Val Acc: 56.26%\n",
            "Epoch [46/200], Train Loss: 1.1280, Val Acc: 57.85%\n",
            "Epoch [47/200], Train Loss: 1.1227, Val Acc: 54.27%\n",
            "Epoch [48/200], Train Loss: 1.1019, Val Acc: 54.27%\n",
            "Epoch [49/200], Train Loss: 1.1106, Val Acc: 54.27%\n",
            "Epoch [50/200], Train Loss: 1.1044, Val Acc: 56.26%\n",
            "Epoch [51/200], Train Loss: 1.1149, Val Acc: 55.67%\n",
            "Epoch [52/200], Train Loss: 1.0951, Val Acc: 57.85%\n",
            "Epoch [53/200], Train Loss: 1.0807, Val Acc: 56.26%\n",
            "Epoch [54/200], Train Loss: 1.0831, Val Acc: 57.26%\n",
            "Epoch [55/200], Train Loss: 1.0726, Val Acc: 58.85%\n",
            "Epoch [56/200], Train Loss: 1.0530, Val Acc: 56.86%\n",
            "Epoch [57/200], Train Loss: 1.0800, Val Acc: 54.47%\n",
            "Epoch [58/200], Train Loss: 1.0480, Val Acc: 57.26%\n"
          ]
        }
      ],
      "source": [
        "def train_model_with_early_stopping(model, train_loader, val_loader, model_name, learning_rate=0.001, patience=70,\n",
        "                                    epochs=100):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Training the model:\", model_name)\n",
        "\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    early_stopping_counter = 0\n",
        "    best_epoch = 0\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "\n",
        "        for batch_index, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_accuracy = evaluate_model(model, val_loader)\n",
        "\n",
        "        average_train_loss = epoch_train_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {average_train_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "        train_losses.append(average_train_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            torch.save(model.state_dict(), \"./\" + model_name + \"_weights.pth\")\n",
        "            best_val_accuracy = val_accuracy\n",
        "            early_stopping_counter = 0\n",
        "            best_epoch = epoch\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"Early stopping triggered on epoch: \", epoch)\n",
        "            print(\"Best weights on epoch: \", best_epoch)\n",
        "            break\n",
        "\n",
        "    return train_losses, val_accuracies\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    val_accuracy = 100.0 * correct / total\n",
        "    return val_accuracy\n",
        "\n",
        "\n",
        "model = SimpleCNNModelWithAll()\n",
        "patience = 30\n",
        "train_losses, val_accuracies = train_model_with_early_stopping(model, train_loader, val_loader,\n",
        "                                                                model_name=\"cnn_early_stopping\", epochs=200,patience=patience)\n",
        "\n",
        "print(\"Train Losses:\", train_losses)\n",
        "print(\"Validation Accuracies:\", val_accuracies)\n",
        "\n",
        "model = load_model(model, \"cnn_early_stopping\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"early stopping test accuracy:\",evaluate_model(model, test_loader))"
      ]
    }
  ]
}